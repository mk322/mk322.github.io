Hello! I'm a passionate researcher in the field of Natural Language Processing (NLP), currently working as a Research Assistant under the guidance of Dr. Xiao-Yang Liu at Columbia University. My academic and research journey has taken me through some fascinating areas of computer science and linguistics, leading to a focus on advancing the capabilities and reliability of NLP systems.

Education
I earned my Bachelor of Science from the prestigious Paul G. Allen School of Computer Science at the University of Washington, Seattle. My time there was not just about academic learning; it was also a period of rich, practical experience. I had the opportunity to work closely with Prof. Luke Zettlemoyer and Terra Blevins, who provided invaluable mentorship and guidance in my early research endeavors.

Professional Experience
Expanding my horizons, I ventured into a research internship with Dr. Huaxiu Yao at the University of North Carolina (UNC). This experience was pivotal, deepening my understanding of NLP and its real-world applications.

Current Role
Now, at Columbia University, I am delving into groundbreaking NLP research. Working with Dr. Xiao-Yang Liu has been an enlightening experience, pushing the boundaries of my knowledge and skills in artificial intelligence and machine learning.

Research Interests
My primary research interests lie at the intersection of technology and language, focusing on:

- Trustworthy NLP, Hallucination, Factuality: I am fascinated by the challenges of ensuring the reliability and truthfulness of NLP outputs. My work aims to mitigate issues like hallucination in generated text and enhance the factuality of language models.

- Multilingual NLP: With the digital world erasing geographical boundaries, I believe in the power of multilingual NLP to bridge communication gaps. I'm working on models that can understand and process multiple languages, bringing down linguistic barriers in information access.

- Reasoning in Large Language Models: My research also encompasses enhancing the reasoning capabilities of large language models. The goal is to develop models that not only understand or translate text but can reason, infer, and make logical conclusions, thereby simulating a more human-like understanding of language and context.

As I continue this exciting journey in NLP, I am always eager to collaborate, share insights, and learn from others in this dynamic field. Feel free to reach out to me for collaborations, discussions, or even a casual chat about the future of language processing!
